{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45907aa0-e22a-45cd-b192-55b42c5f20c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T19:22:52.350003Z",
     "iopub.status.busy": "2025-05-12T19:22:52.349162Z",
     "iopub.status.idle": "2025-05-12T19:23:04.181132Z",
     "shell.execute_reply": "2025-05-12T19:23:04.180381Z",
     "shell.execute_reply.started": "2025-05-12T19:22:52.349975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import uuid\n",
    "import subprocess\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "YOLO_MODEL_PATH = \"/home/jupyter/datasphere/project/train18_best_model.pt\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PEDESTRIAN_CLASSES = [4]\n",
    "YOLO_CONFIDENCE_THRESHOLD = 0.3\n",
    "ULTRALYTICS_OUTPUT_DIR = \"/home/jupyter/datasphere/project/runs/track\"\n",
    "\n",
    "temp_input_video_path = \"/home/jupyter/datasphere/project/MOT20-01-raw.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb441d2f-e9e9-4ea3-a73e-a1b1cdbf9977",
   "metadata": {},
   "source": [
    "### Подсчет среднего потока пешеходов через кластеризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd21c7f1-8ebc-4a4d-838e-1d23e11035a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T17:40:33.894467Z",
     "iopub.status.busy": "2025-05-12T17:40:33.893316Z",
     "iopub.status.idle": "2025-05-12T17:40:33.962672Z",
     "shell.execute_reply": "2025-05-12T17:40:33.961988Z",
     "shell.execute_reply.started": "2025-05-12T17:40:33.894437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YoloVideoFlowAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = DEVICE\n",
    "        self.model = YOLO(YOLO_MODEL_PATH)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "    def process_and_track_with_clusters(self, input_path: str):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        trajectories = defaultdict(list)\n",
    "        detections = defaultdict(list)\n",
    "\n",
    "        frame_idx = 0\n",
    "        results = self.model.track(\n",
    "            source=input_path,\n",
    "            tracker='bytetrack.yaml',\n",
    "            classes=PEDESTRIAN_CLASSES,\n",
    "            conf=YOLO_CONFIDENCE_THRESHOLD,\n",
    "            stream=True,\n",
    "            device=self.device,\n",
    "            half=True\n",
    "        )\n",
    "\n",
    "        for res in results:\n",
    "            ret, _ = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if res.boxes is not None:\n",
    "                ids = res.boxes.id.int().cpu().tolist()\n",
    "                xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "                for track_id, box in zip(ids, xyxy):\n",
    "                    x1, y1, x2, y2 = box.astype(int)\n",
    "                    cx, cy = (x1+x2)//2, (y1+y2)//2\n",
    "\n",
    "                    trajectories[track_id].append((frame_idx, cx, cy))\n",
    "                    detections[frame_idx].append((track_id, (x1, y1, x2, y2)))\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        features = []\n",
    "        track_ids = []\n",
    "        for t_id, pts in trajectories.items():\n",
    "            start = np.array(pts[0][1:])\n",
    "            end   = np.array(pts[-1][1:])\n",
    "            features.append(end - start)\n",
    "            track_ids.append(t_id)\n",
    "        features = np.vstack(features)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0).fit(features)\n",
    "        labels = {tid: lbl for tid, lbl in zip(track_ids, kmeans.labels_)}\n",
    "\n",
    "        cluster_colors = {\n",
    "            -1: (128, 128, 128),\n",
    "            0: (0, 255, 0),\n",
    "            1: (0, 0, 255),\n",
    "            2: (255, 0, 0),\n",
    "            3: (255, 255, 0),\n",
    "            4: (255, 0, 255),\n",
    "            5: (0, 255, 255)\n",
    "        }\n",
    "\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out_path = input_path.replace('.mp4', '_clusters.mp4')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
    "\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            for track_id, box in detections.get(frame_idx, []):\n",
    "                x1, y1, x2, y2 = box\n",
    "                lbl = labels.get(track_id, None)\n",
    "                color = cluster_colors.get(lbl, (255,255,255))\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, str(track_id), (x1, y1-5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            for track_id, pts in trajectories.items():\n",
    "                lbl = labels.get(track_id, None)\n",
    "                color = cluster_colors.get(lbl, (255,255,255))\n",
    "                recent = [p for p in pts if p[0] <= frame_idx][-10:]\n",
    "                for i in range(1, len(recent)):\n",
    "                    _, x0, y0 = recent[i-1]\n",
    "                    _, x1, y1 = recent[i]\n",
    "                    cv2.line(frame, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            writer.write(frame)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "\n",
    "        return out_path, trajectories, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cd932-87a2-4af8-a483-7b34b24af951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = YoloVideoFlowAnalyzer()\n",
    "video, trajs, labels = processor.process_and_track_with_clusters(\"/home/jupyter/datasphere/project/MOT20-01-raw.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf4542-9521-4677-9dd0-9e619ba97d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T17:43:51.599362Z",
     "iopub.status.busy": "2025-05-12T17:43:51.598409Z",
     "iopub.status.idle": "2025-05-12T17:43:51.633962Z",
     "shell.execute_reply": "2025-05-12T17:43:51.633227Z",
     "shell.execute_reply.started": "2025-05-12T17:43:51.599326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пешеходов в каждом кластере:\n",
      "Кластер 0: 283 пешеходов\n",
      "Кластер 1: 21 пешеходов\n"
     ]
    }
   ],
   "source": [
    "cluster_counts = Counter(labels.values())\n",
    "\n",
    "print(\"Количество пешеходов в каждом кластере:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Кластер {cluster_id}: {count} пешеходов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b96baf-3e56-43cf-851b-e36ddce3326b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708089da-34a0-4421-8b46-33ccb4c9bf28",
   "metadata": {},
   "source": [
    "### Подсчет среднего потока пешеходов через оптический поток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaadd964-fa3a-413d-9601-728bcec14107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T19:31:07.105996Z",
     "iopub.status.busy": "2025-05-12T19:31:07.104858Z",
     "iopub.status.idle": "2025-05-12T19:31:07.147108Z",
     "shell.execute_reply": "2025-05-12T19:31:07.146273Z",
     "shell.execute_reply.started": "2025-05-12T19:31:07.105956Z"
    }
   },
   "outputs": [],
   "source": [
    "class YoloOpticalFlowAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = DEVICE\n",
    "        self.model = YOLO(YOLO_MODEL_PATH)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "    def analyze_with_optical_flow(self, input_path: str):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        ret, prev_frame = cap.read()\n",
    "        if not ret:\n",
    "            raise RuntimeError(\"Cannot read first frame\")\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        flow_vectors = []\n",
    "        positions = []\n",
    "        frame_vectors = []\n",
    "        frame_positions = []\n",
    "        frame_idx = 0\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out_path = input_path.replace('.mp4', '_optflow_clusters.mp4')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
    "\n",
    "        results = self.model.track(\n",
    "            source=input_path,\n",
    "            tracker='bytetrack.yaml',\n",
    "            classes=PEDESTRIAN_CLASSES,\n",
    "            conf=YOLO_CONFIDENCE_THRESHOLD,\n",
    "            stream=True,\n",
    "            device=DEVICE,\n",
    "            half=True\n",
    "        )\n",
    "\n",
    "        for res in results:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            prev_gray = gray\n",
    "\n",
    "            current_vectors = []\n",
    "            current_positions = []\n",
    "\n",
    "            if res.boxes is not None:\n",
    "                ids = res.boxes.id.int().cpu().tolist()\n",
    "                xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "                for track_id, box in zip(ids, xyxy):\n",
    "                    x1, y1, x2, y2 = box.astype(int)\n",
    "                    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                    if 0 <= cx < width and 0 <= cy < height:\n",
    "                        dx, dy = flow[cy, cx]\n",
    "                        current_vectors.append([dx, dy])\n",
    "                        current_positions.append((cx, cy))\n",
    "\n",
    "            flow_vectors.extend(current_vectors)\n",
    "            positions.extend(current_positions)\n",
    "            frame_vectors.append(current_vectors)\n",
    "            frame_positions.append(current_positions)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        flow_vectors = np.array(flow_vectors)\n",
    "        if len(flow_vectors) == 0:\n",
    "            raise ValueError(\"No optical flow vectors extracted.\")\n",
    "\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0).fit(flow_vectors)\n",
    "        all_labels = kmeans.labels_\n",
    "\n",
    "        cluster_colors = {\n",
    "            0: (0, 255, 0),\n",
    "            1: (0, 0, 255),\n",
    "            2: (255, 0, 0),\n",
    "            3: (255, 255, 0),\n",
    "            4: (255, 0, 255),\n",
    "            5: (0, 255, 255)\n",
    "        }\n",
    "\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        label_idx = 0\n",
    "        frame_idx = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame_idx >= len(frame_vectors):\n",
    "                break\n",
    "\n",
    "            for (cx, cy), (dx, dy) in zip(frame_positions[frame_idx], frame_vectors[frame_idx]):\n",
    "                if label_idx >= len(all_labels):\n",
    "                    break\n",
    "                lbl = all_labels[label_idx]\n",
    "                color = cluster_colors.get(lbl, (255, 255, 255))\n",
    "                end_point = (int(cx + dx * 10), int(cy + dy * 10))\n",
    "                cv2.arrowedLine(frame, (cx, cy), end_point, color, 2, tipLength=0.3)\n",
    "                label_idx += 1\n",
    "\n",
    "            writer.write(frame)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "\n",
    "        return out_path, all_labels, flow_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b836f5b-e10f-4a3b-b6d1-6a88028dd1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = YoloOpticalFlowAnalyzer()\n",
    "video_path, labels, vectors = processor.analyze_with_optical_flow(\"/home/jupyter/datasphere/project/MOT20-01-raw.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f6a218-26cb-43cb-a3ac-85b9a0b9574e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T19:36:28.062874Z",
     "iopub.status.busy": "2025-05-12T19:36:28.061904Z",
     "iopub.status.idle": "2025-05-12T19:36:28.096605Z",
     "shell.execute_reply": "2025-05-12T19:36:28.095905Z",
     "shell.execute_reply.started": "2025-05-12T19:36:28.062840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пешеходов в каждом кластере:\n",
      "Кластер 1: 2192 пешеходов\n",
      "Кластер 0: 9215 пешеходов\n"
     ]
    }
   ],
   "source": [
    "cluster_counts = Counter(labels)\n",
    "\n",
    "print(\"Количество пешеходов в каждом кластере:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Кластер {cluster_id}: {count} пешеходов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994197c-53ed-4007-91d3-19a0c98b58b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77db557f-10da-4780-9957-ea895199fd7f",
   "metadata": {},
   "source": [
    "### Подсчет среднего потока пешеходов через координаты кадров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a13a5-6dde-4b52-bf5f-9aff28ccdfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:07:38.984615Z",
     "iopub.status.busy": "2025-05-12T20:07:38.983642Z",
     "iopub.status.idle": "2025-05-12T20:07:39.023051Z",
     "shell.execute_reply": "2025-05-12T20:07:39.022119Z",
     "shell.execute_reply.started": "2025-05-12T20:07:38.984575Z"
    }
   },
   "source": [
    "Подсчет количества пешеходов, направление которых влево, вправо, вниз или вверх."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34357a2c-7cf8-4d0e-a041-91c23cb07ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:02:17.627233Z",
     "iopub.status.busy": "2025-05-12T20:02:17.626552Z",
     "iopub.status.idle": "2025-05-12T20:02:17.676765Z",
     "shell.execute_reply": "2025-05-12T20:02:17.676042Z",
     "shell.execute_reply.started": "2025-05-12T20:02:17.627204Z"
    }
   },
   "outputs": [],
   "source": [
    "class YoloVideoFlowAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = DEVICE\n",
    "        self.model = YOLO(YOLO_MODEL_PATH)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "    def process_and_track_with_direction_clusters(self, input_path: str):\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        trajectories = defaultdict(list)\n",
    "        detections = defaultdict(list)\n",
    "\n",
    "        frame_idx = 0\n",
    "        results = self.model.track(\n",
    "            source=input_path,\n",
    "            tracker='bytetrack.yaml',\n",
    "            classes=PEDESTRIAN_CLASSES,\n",
    "            conf=YOLO_CONFIDENCE_THRESHOLD,\n",
    "            stream=True,\n",
    "            device=DEVICE,\n",
    "            half=True\n",
    "        )\n",
    "\n",
    "        for res in results:\n",
    "            ret, _ = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if res.boxes is not None:\n",
    "                ids = res.boxes.id.int().cpu().tolist()\n",
    "                xyxy = res.boxes.xyxy.cpu().numpy()\n",
    "                for track_id, box in zip(ids, xyxy):\n",
    "                    x1, y1, x2, y2 = box.astype(int)\n",
    "                    cx, cy = (x1+x2)//2, (y1+y2)//2\n",
    "                    trajectories[track_id].append((frame_idx, cx, cy))\n",
    "                    detections[frame_idx].append((track_id, (x1, y1, x2, y2)))\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        direction_labels = {}\n",
    "        direction_counts = {\"left\": 0, \"right\": 0, \"up\": 0, \"down\": 0}\n",
    "\n",
    "        for t_id, pts in trajectories.items():\n",
    "            start_x, start_y = pts[0][1:]\n",
    "            end_x, end_y = pts[-1][1:]\n",
    "            dx = end_x - start_x\n",
    "            dy = end_y - start_y\n",
    "\n",
    "            if abs(dx) > abs(dy):\n",
    "                direction = \"right\" if dx > 0 else \"left\"\n",
    "            else:\n",
    "                direction = \"down\" if dy > 0 else \"up\"\n",
    "\n",
    "            direction_labels[t_id] = direction\n",
    "            direction_counts[direction] += 1\n",
    "\n",
    "        print(\"Direction counts:\", direction_counts)\n",
    "\n",
    "        direction_colors = {\n",
    "            \"left\": (0, 0, 255),\n",
    "            \"right\": (0, 255, 0),\n",
    "            \"up\": (255, 0, 0),\n",
    "            \"down\": (255, 255, 0),\n",
    "        }\n",
    "\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out_path = input_path.replace('.mp4', '_direction_clusters.mp4')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
    "\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            for track_id, box in detections.get(frame_idx, []):\n",
    "                x1, y1, x2, y2 = box\n",
    "                direction = direction_labels.get(track_id, \"unknown\")\n",
    "                color = direction_colors.get(direction, (255, 255, 255))\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, f\"{track_id} {direction}\", (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            for track_id, pts in trajectories.items():\n",
    "                direction = direction_labels.get(track_id, \"unknown\")\n",
    "                color = direction_colors.get(direction, (255, 255, 255))\n",
    "                recent = [p for p in pts if p[0] <= frame_idx][-10:]\n",
    "                for i in range(1, len(recent)):\n",
    "                    _, x0, y0 = recent[i-1]\n",
    "                    _, x1, y1 = recent[i]\n",
    "                    cv2.line(frame, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            writer.write(frame)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "\n",
    "        return out_path, trajectories, direction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dfeec6-fa70-4758-afc2-b6b082976c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = YoloVideoFlowAnalyzer()\n",
    "video, trajs, directions = processor.process_and_track_with_direction_clusters(temp_input_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af198c0-7e04-436a-b0bf-2ada94e39988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:06:05.002096Z",
     "iopub.status.busy": "2025-05-12T20:06:05.001239Z",
     "iopub.status.idle": "2025-05-12T20:06:05.043921Z",
     "shell.execute_reply": "2025-05-12T20:06:05.043087Z",
     "shell.execute_reply.started": "2025-05-12T20:06:05.002068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пешеходов в каждом кластере:\n",
      "Кластер down: 58 пешеходов\n",
      "Кластер right: 120 пешеходов\n",
      "Кластер left: 53 пешеходов\n",
      "Кластер up: 73 пешеходов\n"
     ]
    }
   ],
   "source": [
    "cluster_counts = Counter(directions.values())\n",
    "\n",
    "print(\"Количество пешеходов в каждом кластере:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Кластер {cluster_id}: {count} пешеходов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df12593-ae92-47d3-8d66-9d0f21f04930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
